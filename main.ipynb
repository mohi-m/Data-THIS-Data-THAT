{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synchrony Datathalon 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load all the datasets\n",
    "account_df = pd.read_csv('data/account_dim_20250325.csv')\n",
    "fraud_case_df = pd.read_csv('data/fraud_claim_case_20250325.csv')\n",
    "fraud_tran_df = pd.read_csv('data/fraud_claim_tran_20250325.csv')\n",
    "rams_df = pd.read_csv('data/rams_batch_cur_20250325.csv')\n",
    "statement_df = pd.read_csv('data/statement_fact_20250325.csv')\n",
    "syfid_df = pd.read_csv('data/syf_id_20250325.csv')\n",
    "transaction_df = pd.read_csv('data/transaction_fact_20250325.csv')\n",
    "wrld_transaction_df = pd.read_csv('data/wrld_stor_tran_fact_20250325.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Process functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the RAMS dataset\n",
    "def clean_rams(df):\n",
    "    # Convert date column to datetime format\n",
    "    df['cu_processing_date'] = pd.to_datetime(df['cu_processing_date'])\n",
    "\n",
    "    # Keep only the latest processing date for each account\n",
    "    latest_df = df.sort_values(by=['cu_processing_date'], ascending=False).drop_duplicates(subset=['cu_account_nbr'], keep='first')\n",
    "\n",
    "    # drop the ca_cash_bal_pct_crd_line column\n",
    "    df = df.drop(columns=['ca_cash_bal_pct_crd_line'])\n",
    "    # irrelevant column as the values are all 0\n",
    "\n",
    "    #drop the cu_nbr_days_dlq column\n",
    "    df = df.drop(columns=['cu_nbr_days_dlq'])\n",
    "    #redundant as theres a simialr column with months which is more useful\n",
    "\n",
    "    #drop the ca_cash_bal_pct_cash_line column\n",
    "    df = df.drop(columns=['ca_cash_bal_pct_cash_line'])\n",
    "\n",
    "    #For all values in column cu_crd_bureau_scr , replace the value 0 with median of the column cu_crd_bureau_scr\n",
    "    df['cu_crd_bureau_scr'] = df['cu_crd_bureau_scr'].replace(0, df['cu_crd_bureau_scr'].median())\n",
    "\n",
    "    #drop the column cu_next_crd_line_rev_date\n",
    "    df = df.drop(columns=['cu_next_crd_line_rev_date'])\n",
    "    #irrelevant column as majority values are 0\n",
    "\n",
    "    #dropping columns\n",
    "    useless_col = [\n",
    "        'cu_cur_balance',\n",
    "        'ca_mob',\n",
    "        'cu_rnd_nbr',\n",
    "        'rb_crd_gr_new_crd_gr',\n",
    "        'cu_processing_date',\n",
    "        'mo_tot_sales_array_1',\n",
    "        'mo_tot_sales_array_2',\n",
    "        'mo_tot_sales_array_3',\n",
    "        'mo_tot_sales_array_4',\n",
    "        'mo_tot_sales_array_5',\n",
    "        'mo_tot_sales_array_6'\n",
    "    ]\n",
    "\n",
    "    #Droping the above generated columns\n",
    "    df = df.drop(columns=useless_col)\n",
    "\n",
    "    # for the values 999999999999999 in cu_cash_line_am replace them with 20% of corresponding valur of cu_crd_bureau_scr column \n",
    "    df['cu_cash_line_am'] = df.apply(\n",
    "        lambda row: row['cu_crd_bureau_scr'] * 0.2 if row['cu_cash_line_am'] == 999999999999999 else row['cu_cash_line_am'],\n",
    "        axis=1\n",
    "    )\n",
    "    # round it off to 2 decimal places\n",
    "    df['cu_cash_line_am'] = df['cu_cash_line_am'].round(2)\n",
    "\n",
    "    #drop duplicate rows with duplicate values in the column cu_account_nbr\n",
    "    df = df.drop_duplicates(subset=['cu_account_nbr'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the account dataset\n",
    "def pre_process_accounts(df):\n",
    "    account_df = df.copy()\n",
    "\n",
    "    # replace all \\\\\\\"\\\\\\\" values with NaN\n",
    "    account_df.replace(r'\\\\\\\"', np.nan, regex=True, inplace=True)\n",
    "\n",
    "    # drop duplicate rows\n",
    "    account_df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # drop useless columns\n",
    "    useless_column  = ['date_in_collection', # All values are null\n",
    "                       'special_finance_charge_ind', # All values are null\n",
    "                       'card_activation_date', # Redundant since we have card_activation_flag\n",
    "                       'ebill_ind', # Not usefull for computing credit_line increase\n",
    "                       'overlimit_type_flag', # All values are 0\n",
    "                       'external_status_reason_code' # Redundant with ext_status_reason_cd_desc\n",
    "                       ]\n",
    "    account_df.drop(columns=useless_column, inplace=True, errors='ignore')\n",
    "\n",
    "    # convert date columns to datetime\n",
    "    date_columns = ['open_date']\n",
    "    for col in date_columns:\n",
    "        account_df[col] = pd.to_datetime(account_df[col], errors='coerce')\n",
    "\n",
    "    # update the card_activation_flag to 0 or 1. Currently it has the following values: 0, 7, 8 and nan. 0 mean activated and the rest are not activated.\n",
    "    account_df['card_activation_flag'] = account_df['card_activation_flag'].replace({'0': 1, '7': 0, '8': 0, np.nan: 0})\n",
    "\n",
    "    # Create empty columns for each month\n",
    "    for i in range(12):\n",
    "        account_df[f\"Month_{i+1}\"] = account_df['payment_hist_1_12_mths'].apply(lambda x: x[i] if pd.notna(x) and i < len(x) else np.nan)\n",
    "\n",
    "    for i in range(12):\n",
    "        account_df[f\"Month_{i+13}\"] = account_df['payment_hist_13_24_mths'].apply(lambda x: x[i] if pd.notna(x) and i < len(x) else np.nan)\n",
    "\n",
    "    # Drop the original payment history columns\n",
    "    account_df.drop(columns=['payment_hist_1_12_mths', 'payment_hist_13_24_mths'], inplace=True)\n",
    "\n",
    "    # Update the employee_code column. Currently it has the following values: H, Y and empty/na/null. Y mean employee, H means high spending customer and empty/na/null means normal customer. I want a separate column for high spending customer and employee. The rest are normal customers.\n",
    "    account_df['high_spending_customer'] = account_df['employee_code'].replace({'H': 1, 'Y': 0, '': 0, np.nan: 0})\n",
    "    account_df['employee_code'] = account_df['employee_code'].replace({'Y': 1, 'H': 0, '': 0, np.nan: 0})\n",
    "\n",
    "    return account_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply all the preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mohi2\\AppData\\Local\\Temp\\ipykernel_50692\\823880188.py:24: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  account_df[col] = pd.to_datetime(account_df[col], errors='coerce')\n",
      "C:\\Users\\mohi2\\AppData\\Local\\Temp\\ipykernel_50692\\823880188.py:27: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  account_df['card_activation_flag'] = account_df['card_activation_flag'].replace({'0': 1, '7': 0, '8': 0, np.nan: 0})\n",
      "C:\\Users\\mohi2\\AppData\\Local\\Temp\\ipykernel_50692\\823880188.py:40: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  account_df['high_spending_customer'] = account_df['employee_code'].replace({'H': 1, 'Y': 0, '': 0, np.nan: 0})\n",
      "C:\\Users\\mohi2\\AppData\\Local\\Temp\\ipykernel_50692\\823880188.py:41: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  account_df['employee_code'] = account_df['employee_code'].replace({'Y': 1, 'H': 0, '': 0, np.nan: 0})\n"
     ]
    }
   ],
   "source": [
    "updated_rams_df = clean_rams(rams_df)\n",
    "updated_accounts_df = pre_process_accounts(account_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
